# -*- coding: utf-8 -*-
"""Tanzania Toursim Capston Project Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_oFIyJLEYzKVgh8SN6iVN6jWXyhWctF

# Tanzania Tourism Prediction by Pycon Tanzania Community Project

## import libraries for data manipulation
"""



import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()
import warnings 
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)

"""## import the datasets"""

from google.colab import files
uploaded = files.upload()

variable_df = pd.read_csv('VariableDefinitions.csv')
train_data = pd.read_csv("Train (2).csv")
test_data = pd.read_csv("Test.csv")
submission_df = pd.read_csv('SampleSubmission.csv')

"""Variables in the data"""

variable_df

submission_df

print(train_data.shape)
train_data.head()

print(test_data.shape)
test_data.head()

"""create copies of data"""

train_data_copy = train_data.copy()
test_data_copy = test_data.copy()

"""### Data Cleaning

Duplicates
"""

print("Train Duplicates: ", train_data.duplicated().sum())
print("Test Duplicates: ", test_data.duplicated().sum())

"""Missing values"""

train_data.isnull().sum()

plt.figure(figsize=(20, 4))
sns.heatmap(train_data.isnull(), cmap='BuPu')

all_null_values = (train_data.isnull().sum() / len(train_data)) * 100
all_null_values = round(all_null_values.sort_values(ascending=False), 2)
missing_data = pd.DataFrame({'Percente of null values(train)' :all_null_values})
missing_data

plt.figure(figsize=(20, 4))
sns.heatmap(test_data.isnull(), cmap='BuPu')

all_null_values = (test_data.isnull().sum() / len(train_data)) * 100
all_null_values = round(all_null_values.sort_values(ascending=False), 2)
missing_data = pd.DataFrame({'Percente of null values(train)' :all_null_values})
missing_data

"""Travel categories

"""

train_data['travel_with'].value_counts()

test_data['travel_with'].value_counts()

train_data['most_impressing'].value_counts()

test_data['most_impressing'].value_counts()

"""### Fill missing values 

"""

train_data['travel_with'].fillna('Alone', inplace=True)
test_data['travel_with'].fillna('Alone', inplace=True)

train_data['most_impressing'].fillna('Friendly People', inplace=True)
test_data['most_impressing'].fillna('Friendly People', inplace=True)

train_data.isnull().sum()

train_data.fillna(train_data.median(), inplace=True)
test_data.fillna(test_data.median(), inplace=True)

train_data.isnull().sum().sum()
test_data.isnull().sum().sum()

"""### EDA"""

train_data.info()

### Descriptive statistics for 
train_data.describe()

train_data.describe(include='object')

df = train_data['country'].value_counts().nlargest(20)
df = pd.DataFrame(df)
sns.barplot(x=df.index, y=df.country)
plt.xticks(rotation= 90)

sns.countplot(x=test_data['age_group'])

sns.barplot(x=test_data['age_group'], y=train_data["total_cost"])

sns.barplot(x=test_data['main_activity'], y=train_data["total_cost"])
plt.xticks(rotation=90)

sns.barplot(x=test_data['purpose'], y=train_data["total_cost"])
plt.xticks(rotation=90)

sns.distplot(train_data['night_mainland'])

sns.distplot(train_data['night_zanzibar'])

sns.distplot(train_data['total_cost'])

sns.scatterplot(x=train_data.night_mainland, y=train_data.total_cost)

sns.scatterplot(x=train_data.night_zanzibar, y=train_data.total_cost)

"""### Freature engineering and transformation"""

full_data = train_data.append(test_data, ignore_index=True)
full_data.shape

full_data.country.value_counts(normalize=True)*100

series = full_data['country'].value_counts()
mask = (full_data['country'].value_counts(normalize=True) * 100).lt(5)
# To replace df['column'] use np.where I.e 
full_data['country'] = np.where(full_data['country'].isin(series[mask].index),'Other',full_data['country'])

full_data.country.value_counts(normalize=True)*100

sns.barplot(x=full_data['country'], y=full_data["total_cost"])
plt.xticks(rotation=90)

full_data.head(3)

full_data.info_source.value_counts(normalize=True)*100

series = full_data['info_source'].value_counts()
mask = (full_data['info_source'].value_counts(normalize=True) * 100).lt(5)
# To replace df['column'] use np.where I.e 
full_data['info_source'] = np.where(full_data['info_source'].isin(series[mask].index),'others',full_data['info_source'])

full_data.info_source.value_counts(normalize=True)*100

sns.barplot(y=full_data['info_source'], x=full_data["total_cost"])
# plt.xticks(rotation=90)

full_data.main_activity.value_counts(normalize=True)*100

series = full_data['main_activity'].value_counts()
mask = (full_data['main_activity'].value_counts(normalize=True) * 100).lt(7)
# To replace df['column'] use np.where I.e 
full_data['main_activity'] = np.where(full_data['main_activity'].isin(series[mask].index),'others',full_data['main_activity'])

full_data.main_activity.value_counts(normalize=True)*100

full_data.travel_with.value_counts(normalize=True)*100

full_data.most_impressing.value_counts(normalize=True)*100

series = full_data['most_impressing'].value_counts()
mask = (full_data['most_impressing'].value_counts(normalize=True) * 100).lt(10)
# To replace df['column'] use np.where I.e 
full_data['most_impressing'] = np.where(full_data['most_impressing'].isin(series[mask].index),'others',full_data['most_impressing'])

full_data.most_impressing.value_counts(normalize=True)*100

full_data.head(3)

full_data.groupby('travel_with')[['total_female', 'total_male']].agg(['min', 'max'])

"""#### Categorical Encoding"""

cat_df = full_data.select_dtypes(include='object')
print(cat_df.shape)
cat_df.drop('ID', 1, inplace=True)
cat_df.head()

"""#### Using label enconder"""

#Label Encoding for object to numeric conversion
from sklearn.preprocessing import LabelEncoder

def encode():
    le = LabelEncoder()

    for col in cat_df:
        full_data[col] = le.fit_transform(full_data[col].astype(str))

    print (full_data.info())
encode()

full_data.head()

"""#### Machine learning modeling"""

train_data.tail(2)

test_data.head(2)

train_data_1 = full_data.loc[:4808]

train_data_1.tail(2)

fd = full_data.loc[4807:4810]
fd

test_data_1 = full_data.loc[4809:]
test_data_1.head()

"""#### Libraries for ML"""

## splitting data
from sklearn.model_selection import train_test_split, KFold, cross_val_score

# Regression algorithms 
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# regression metrics 
from sklearn.metrics import mean_absolute_error, r2_score

"""#### Splitting train_data_1 using t-t-s"""

X = train_data_1.drop(['total_cost', 'ID'], axis=1)
y = train_data_1['total_cost']

X.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

def train_models():
  models = []
  models.append(('Linear Reg', LinearRegression()))
  models.append(('Ridge Reg', Ridge()))
  models.append(('Lass Reg', Lasso()))
  models.append(('ElasticNet Reg', ElasticNet()))
  models.append(('SVR Reg', SVR()))
  models.append(('KNN Reg', KNeighborsRegressor()))
  models.append(('DT Reg', DecisionTreeRegressor()))
  models.append(('RF Reg', RandomForestRegressor()))
  models.append(('GB Reg', GradientBoostingRegressor()))
  models.append(('Ada Reg', AdaBoostRegressor()))
  models.append(('EXT Reg', ExtraTreesRegressor()))
  models.append(('XGB Reg', XGBRegressor()))
  models.append(('LGM Reg', LGBMRegressor()))

  r2_train = []
  r2_test =[]
  mae = []
  mse = []
  rmse = []
  mape = []
  names = []

  for name, model in models:
    model_fit = model.fit(X_train, y_train)
    y_pred = model_fit.predict(X_test)
    r2_train.append(model_fit.score(X_train, y_train))
    r2_test.append(model_fit.score(X_test, y_test))
    mae.append(mean_absolute_error(y_test, y_pred))
    names.append(name)

  model_dict = {'Algo':names, 'R_squared_train':r2_train, 'R_squared_test':r2_test,'MAE':mae}
  Model_df = pd.DataFrame(model_dict)

  return Model_df

ml_df = train_models()

ml_df

"""#### scalling """

from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

def train_models():
  models = []
  models.append(('Linear Reg', LinearRegression()))
  models.append(('Ridge Reg', Ridge()))
  models.append(('Lass Reg', Lasso()))
  models.append(('ElasticNet Reg', ElasticNet()))
  models.append(('SVR Reg', SVR()))
  models.append(('KNN Reg', KNeighborsRegressor()))
  models.append(('DT Reg', DecisionTreeRegressor()))
  models.append(('RF Reg', RandomForestRegressor()))
  models.append(('GB Reg', GradientBoostingRegressor()))
  models.append(('Ada Reg', AdaBoostRegressor()))
  models.append(('EXT Reg', ExtraTreesRegressor()))
  models.append(('XGB Reg', XGBRegressor()))
  models.append(('LGM Reg', LGBMRegressor()))

  r2_train = []
  r2_test =[]
  mae = []
  mse = []
  rmse = []
  mape = []
  names = []

  for name, model in models:
    model_fit = model.fit(X_train_scaled, y_train)
    y_pred = model_fit.predict(X_test_scaled)
    r2_train.append(model_fit.score(X_train_scaled, y_train))
    r2_test.append(model_fit.score(X_test_scaled, y_test))
    mae.append(mean_absolute_error(y_test, y_pred))
    names.append(name)

  model_dict = {'Algo':names, 'R_squared_train':r2_train, 'R_squared_test':r2_test,'MAE':mae}
  Model_df = pd.DataFrame(model_dict)

  return Model_df

ml_df_scaled_data = train_models()
ml_df_scaled_data

xgb_model = XGBRegressor(max_depth=2, n_estimators=100, learning_rate=0.1)
xgb_model.fit(X, y)
print(xgb_model.score(X, y))
# print(xgb_model.score(X_test, y_test))

test_data_1.head()

test = test_data_1.drop(['ID', 'total_cost'], axis=1)

total_cost_pred = xgb_model.predict(test)

submission_df['total_cost'] = total_cost_pred
submission_df

submission_df.to_csv('sub2.csv', index=False)

"""### Feature selection"""

# Perform feature selection using a univariate statistical test
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif # use this for classification tasks
from sklearn.feature_selection import f_regression # use this for regression tasks

kbest = SelectKBest(score_func=f_regression, k=15)
kbest.fit(X_train, y_train)

print("Selected features:", list(X.columns[kbest.get_support()]))
print('\n')
print("Removed features:", list(X.columns[~kbest.get_support()]))

dfscores = pd.DataFrame(kbest.scores_)
dfcolumns = pd.DataFrame(X.columns)

#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
featureScores.nlargest(15,'Score')  #print 10 best features

"""#### training the model using the selected features"""

X_selected = X.drop(['country', 'main_activity', 'night_mainland', 'night_zanzibar', 'payment_mode', 'most_impressing'], axis=1)
X_selected.head()

X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=.25, random_state=42)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

def train_models():
  models = []
  models.append(('Linear Reg', LinearRegression()))
  models.append(('Ridge Reg', Ridge()))
  models.append(('Lass Reg', Lasso()))
  models.append(('ElasticNet Reg', ElasticNet()))
  models.append(('SVR Reg', SVR()))
  models.append(('KNN Reg', KNeighborsRegressor()))
  models.append(('DT Reg', DecisionTreeRegressor()))
  models.append(('RF Reg', RandomForestRegressor()))
  models.append(('GB Reg', GradientBoostingRegressor()))
  models.append(('Ada Reg', AdaBoostRegressor()))
  models.append(('EXT Reg', ExtraTreesRegressor()))
  models.append(('XGB Reg', XGBRegressor()))
  models.append(('LGM Reg', LGBMRegressor()))

  r2_train = []
  r2_test =[]
  mae = []
  mse = []
  rmse = []
  mape = []
  names = []

  for name, model in models:
    model_fit = model.fit(X_train, y_train)
    y_pred = model_fit.predict(X_test)
    r2_train.append(model_fit.score(X_train, y_train))
    r2_test.append(model_fit.score(X_test, y_test))
    mae.append(mean_absolute_error(y_test, y_pred))
    names.append(name)

  model_dict = {'Algo':names, 'R_squared_train':r2_train, 'R_squared_test':r2_test,'MAE':mae}
  Model_df = pd.DataFrame(model_dict)

  return Model_df

ml_df_selected = train_models()
ml_df_selected

"""#### Feature importance """

from yellowbrick.model_selection import CVScores, FeatureImportances 
from yellowbrick.exceptions import YellowbrickValueError

visualizer = FeatureImportances(xgb_model)
visualizer.fit(X, y)        # Fit the data to the visualizer
visualizer.show()

X_selected_1 = X[['tour_arrangement', 'package_transport_int', 'package_accomodation', 
                  'package_transport_tz', 'travel_with', 'package_sightseeing',
                  'age_group', 'total_female', 'total_male']]
X_selected_1.head()

X_train, X_test, y_train, y_test = train_test_split(X_selected_1, y, test_size=.25, random_state=42)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

xgb_model = XGBRegressor(max_depth=2, n_estimators=150, min_child_weight=3, learning_rate=0.1).fit(X_train, y_train)
print("R-squared in train set: {:.3f}%".format(xgb_model.score(X_train, y_train)*100))
print("R-squared in test set: {:.3f}%".format(xgb_model.score(X_test, y_test)*100))

xgb_model = XGBRegressor(max_depth=2, n_estimators=150, min_child_weight=3, learning_rate=0.1).fit(X_selected_1, y)

X_selected_1.columns

test_1 = test_data_1[X_selected_1.columns]
test_1.head()

total_cost_pred = xgb_model.predict(test_1)
submission_df['total_cost'] = total_cost_pred
submission_df.head()

submission_df.to_csv('sub3.csv', index=False)

"""#### Hyperparameter tuning """

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

from sklearn.model_selection import GridSearchCV

xgb1 = XGBRegressor()
parameters = {#'nthread':[4], #when use hyperthread, xgboost may become slower
              #'objective':['reg:linear'],
              #'learning_rate': [0.1, .03], #so called `eta` value
              'max_depth': [4, 5, 6],
              'min_child_weight': [3, 4, 5],
              #'silent': [1, 2],
              'subsample': [0.7, 0.8, 0.9],
              'colsample_bytree': [0.7, 0.8, 0.9],
              'n_estimators': [30, 50, 80]}

xgb_grid = GridSearchCV(xgb1, param_grid=parameters, cv = 3, n_jobs = -1, verbose=True)
xgb_grid.fit(X_train, y_train)

print(xgb_grid.best_score_)
print(xgb_grid.best_params_)

xgb1 = XGBRegressor(colsample_bytree=0.7, max_depth=4, min_child_weight=5, n_estimators=50, subsample=0.7)
xgb1.fit(X, y)

test = test_data_1.drop(['ID', 'total_cost'], axis=1)
total_cost_pred = xgb1.predict(test)
submission_df['total_cost'] = total_cost_pred
submission_df.to_csv('sub5.csv', index=False)

lgbm1 = LGBMRegressor()
parameters = {#'nthread':[4], #when use hyperthread, xgboost may become slower
              #'objective':['reg:linear'],
              #'learning_rate': [0.1, .03], #so called `eta` value
              'max_depth': [4, 5, 6],
              'min_child_weight': [3, 4, 5],
              #'silent': [1, 2],
              'subsample': [0.7, 0.8, 0.9],
              'colsample_bytree': [0.7, 0.8, 0.9],
              'n_estimators': [30, 50, 80]}

lgbm_grid = GridSearchCV(lgbm1, param_grid=parameters, cv = 3, n_jobs = -1, verbose=True)
lgbm_grid.fit(X_train, y_train)

print(lgbm_grid.best_score_)
print(lgbm_grid.best_params_)

lgbm1 = LGBMRegressor(colsample_bytree=0.9, max_depth=5, min_child_weight=3, n_estimators=50, subsample=0.7)
lgbm1.fit(X, y)

test = test_data_1.drop(['ID', 'total_cost'], axis=1)
total_cost_pred = lgbm1.predict(test)
submission_df['total_cost'] = total_cost_pred
submission_df.to_csv('sub6.csv', index=False)

!pip install catboost

from catboost import CatBoostRegressor

cat_model = CatBoostRegressor().fit(X_train, y_train)
print("R-squared in train set: {:.3f}%".format(cat_model.score(X_train, y_train)*100))
print("R-squared in test set: {:.3f}%".format(cat_model.score(X_test, y_test)*100))

cat1 = CatBoostRegressor()
cat1.fit(X, y)

test = test_data_1.drop(['ID', 'total_cost'], axis=1)
total_cost_pred = cat1.predict(test)
submission_df['total_cost'] = total_cost_pred
submission_df.to_csv('sub7.csv', index=False)

